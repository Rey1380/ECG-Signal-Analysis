{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WavLM.ipynb",
      "provenance": [],
      "mount_file_id": "1x0RHfxjHHTn2Mh4vuL9HfbD5eptudh38",
      "authorship_tag": "ABX9TyPuiflBz//OgxeYdKT0j7Q8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rey1380/ECG-Signal-Analysis/blob/main/WavLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------\n",
        "# WavLM: Large-Scale Self-Supervised  Pre-training  for Full Stack Speech Processing (https://arxiv.org/abs/2110.13900.pdf)\n",
        "# Github source: https://github.com/microsoft/unilm/tree/master/wavlm\n",
        "# Copyright (c) 2021 Microsoft\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# Based on fairseq code bases\n",
        "# https://github.com/pytorch/fairseq\n",
        "# --------------------------------------------------------\n",
        "\n",
        "import math\n",
        "import warnings\n",
        "from typing import Dict, Optional, Tuple\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TransposeLast(nn.Module):\n",
        "    def __init__(self, deconstruct_idx=None):\n",
        "        super().__init__()\n",
        "        self.deconstruct_idx = deconstruct_idx\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.deconstruct_idx is not None:\n",
        "            x = x[self.deconstruct_idx]\n",
        "        return x.transpose(-2, -1)\n",
        "\n",
        "\n",
        "class Fp32LayerNorm(nn.LayerNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = F.layer_norm(\n",
        "            input.float(),\n",
        "            self.normalized_shape,\n",
        "            self.weight.float() if self.weight is not None else None,\n",
        "            self.bias.float() if self.bias is not None else None,\n",
        "            self.eps,\n",
        "        )\n",
        "        return output.type_as(input)\n",
        "\n",
        "\n",
        "class Fp32GroupNorm(nn.GroupNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = F.group_norm(\n",
        "            input.float(),\n",
        "            self.num_groups,\n",
        "            self.weight.float() if self.weight is not None else None,\n",
        "            self.bias.float() if self.bias is not None else None,\n",
        "            self.eps,\n",
        "        )\n",
        "        return output.type_as(input)\n",
        "\n",
        "\n",
        "class GradMultiply(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, scale):\n",
        "        ctx.scale = scale\n",
        "        res = x.new(x)\n",
        "        return res\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad):\n",
        "        return grad * ctx.scale, None\n",
        "\n",
        "\n",
        "class SamePad(nn.Module):\n",
        "    def __init__(self, kernel_size, causal=False):\n",
        "        super().__init__()\n",
        "        if causal:\n",
        "            self.remove = kernel_size - 1\n",
        "        else:\n",
        "            self.remove = 1 if kernel_size % 2 == 0 else 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.remove > 0:\n",
        "            x = x[:, :, : -self.remove]\n",
        "        return x\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    \"\"\"Swish function\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Construct an MultiHeadedAttention object.\"\"\"\n",
        "        super(Swish, self).__init__()\n",
        "        self.act = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.act(x)\n",
        "\n",
        "\n",
        "class GLU_Linear(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, glu_type=\"sigmoid\", bias_in_glu=True):\n",
        "        super(GLU_Linear, self).__init__()\n",
        "\n",
        "        self.glu_type = glu_type\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        if glu_type == \"sigmoid\":\n",
        "            self.glu_act = torch.nn.Sigmoid()\n",
        "        elif glu_type == \"swish\":\n",
        "            self.glu_act = Swish()\n",
        "        elif glu_type == \"relu\":\n",
        "            self.glu_act = torch.nn.ReLU()\n",
        "        elif glu_type == \"gelu\":\n",
        "            self.glu_act = torch.nn.GELU()\n",
        "\n",
        "        if bias_in_glu:\n",
        "            self.linear = nn.Linear(input_dim, output_dim * 2, True)\n",
        "        else:\n",
        "            self.linear = nn.Linear(input_dim, output_dim * 2, False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # to be consistent with GLU_Linear, we assume the input always has the #channel (#dim) in the last dimension of the tensor, so need to switch the dimension first for 1D-Conv case\n",
        "        x = self.linear(x)\n",
        "\n",
        "        if self.glu_type == \"bilinear\":\n",
        "            x = (x[:, :, 0:self.output_dim] * x[:, :, self.output_dim:self.output_dim * 2])\n",
        "        else:\n",
        "            x = (x[:, :, 0:self.output_dim] * self.glu_act(x[:, :, self.output_dim:self.output_dim * 2]))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def gelu_accurate(x):\n",
        "    if not hasattr(gelu_accurate, \"_a\"):\n",
        "        gelu_accurate._a = math.sqrt(2 / math.pi)\n",
        "    return (\n",
        "        0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    )\n",
        "\n",
        "\n",
        "def gelu(x: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.nn.functional.gelu(x.float()).type_as(x)\n",
        "\n",
        "\n",
        "def get_activation_fn(activation: str):\n",
        "    \"\"\"Returns the activation function corresponding to `activation`\"\"\"\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    elif activation == \"gelu\":\n",
        "        return gelu\n",
        "    elif activation == \"gelu_fast\":\n",
        "        warnings.warn(\n",
        "            \"--activation-fn=gelu_fast has been renamed to gelu_accurate\"\n",
        "        )\n",
        "        return gelu_accurate\n",
        "    elif activation == \"gelu_accurate\":\n",
        "        return gelu_accurate\n",
        "    elif activation == \"tanh\":\n",
        "        return torch.tanh\n",
        "    elif activation == \"linear\":\n",
        "        return lambda x: x\n",
        "    elif activation == \"glu\":\n",
        "        return lambda x: x\n",
        "    else:\n",
        "        raise RuntimeError(\"--activation-fn {} not supported\".format(activation))\n",
        "\n",
        "\n",
        "def init_bert_params(module):\n",
        "    \"\"\"\n",
        "    Initialize the weights specific to the BERT Model.\n",
        "    This overrides the default initializations depending on the specified arguments.\n",
        "        1. If normal_init_linear_weights is set then weights of linear\n",
        "           layer will be initialized using the normal distribution and\n",
        "           bais will be set to the specified value.\n",
        "        2. If normal_init_embed_weights is set then weights of embedding\n",
        "           layer will be initialized using the normal distribution.\n",
        "        3. If normal_init_proj_weights is set then weights of\n",
        "           in_project_weight for MultiHeadAttention initialized using\n",
        "           the normal distribution (to be validated).\n",
        "    \"\"\"\n",
        "\n",
        "    def normal_(data):\n",
        "        # with FSDP, module params will be on CUDA, so we cast them back to CPU\n",
        "        # so that the RNG is consistent with and without FSDP\n",
        "        data.copy_(\n",
        "            data.cpu().normal_(mean=0.0, std=0.02).to(data.device)\n",
        "        )\n",
        "\n",
        "    if isinstance(module, nn.Linear):\n",
        "        normal_(module.weight.data)\n",
        "        if module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "    if isinstance(module, nn.Embedding):\n",
        "        normal_(module.weight.data)\n",
        "        if module.padding_idx is not None:\n",
        "            module.weight.data[module.padding_idx].zero_()\n",
        "    if isinstance(module, MultiheadAttention):\n",
        "        normal_(module.q_proj.weight.data)\n",
        "        normal_(module.k_proj.weight.data)\n",
        "        normal_(module.v_proj.weight.data)\n",
        "\n",
        "\n",
        "def quant_noise(module, p, block_size):\n",
        "    \"\"\"\n",
        "    Wraps modules and applies quantization noise to the weights for\n",
        "    subsequent quantization with Iterative Product Quantization as\n",
        "    described in \"Training with Quantization Noise for Extreme Model Compression\"\n",
        "\n",
        "    Args:\n",
        "        - module: nn.Module\n",
        "        - p: amount of Quantization Noise\n",
        "        - block_size: size of the blocks for subsequent quantization with iPQ\n",
        "\n",
        "    Remarks:\n",
        "        - Module weights must have the right sizes wrt the block size\n",
        "        - Only Linear, Embedding and Conv2d modules are supported for the moment\n",
        "        - For more detail on how to quantize by blocks with convolutional weights,\n",
        "          see \"And the Bit Goes Down: Revisiting the Quantization of Neural Networks\"\n",
        "        - We implement the simplest form of noise here as stated in the paper\n",
        "          which consists in randomly dropping blocks\n",
        "    \"\"\"\n",
        "\n",
        "    # if no quantization noise, don't register hook\n",
        "    if p <= 0:\n",
        "        return module\n",
        "\n",
        "    # supported modules\n",
        "    assert isinstance(module, (nn.Linear, nn.Embedding, nn.Conv2d))\n",
        "\n",
        "    # test whether module.weight has the right sizes wrt block_size\n",
        "    is_conv = module.weight.ndim == 4\n",
        "\n",
        "    # 2D matrix\n",
        "    if not is_conv:\n",
        "        assert (\n",
        "            module.weight.size(1) % block_size == 0\n",
        "        ), \"Input features must be a multiple of block sizes\"\n",
        "\n",
        "    # 4D matrix\n",
        "    else:\n",
        "        # 1x1 convolutions\n",
        "        if module.kernel_size == (1, 1):\n",
        "            assert (\n",
        "                module.in_channels % block_size == 0\n",
        "            ), \"Input channels must be a multiple of block sizes\"\n",
        "        # regular convolutions\n",
        "        else:\n",
        "            k = module.kernel_size[0] * module.kernel_size[1]\n",
        "            assert k % block_size == 0, \"Kernel size must be a multiple of block size\"\n",
        "\n",
        "    def _forward_pre_hook(mod, input):\n",
        "        # no noise for evaluation\n",
        "        if mod.training:\n",
        "            if not is_conv:\n",
        "                # gather weight and sizes\n",
        "                weight = mod.weight\n",
        "                in_features = weight.size(1)\n",
        "                out_features = weight.size(0)\n",
        "\n",
        "                # split weight matrix into blocks and randomly drop selected blocks\n",
        "                mask = torch.zeros(\n",
        "                    in_features // block_size * out_features, device=weight.device\n",
        "                )\n",
        "                mask.bernoulli_(p)\n",
        "                mask = mask.repeat_interleave(block_size, -1).view(-1, in_features)\n",
        "\n",
        "            else:\n",
        "                # gather weight and sizes\n",
        "                weight = mod.weight\n",
        "                in_channels = mod.in_channels\n",
        "                out_channels = mod.out_channels\n",
        "\n",
        "                # split weight matrix into blocks and randomly drop selected blocks\n",
        "                if mod.kernel_size == (1, 1):\n",
        "                    mask = torch.zeros(\n",
        "                        int(in_channels // block_size * out_channels),\n",
        "                        device=weight.device,\n",
        "                    )\n",
        "                    mask.bernoulli_(p)\n",
        "                    mask = mask.repeat_interleave(block_size, -1).view(-1, in_channels)\n",
        "                else:\n",
        "                    mask = torch.zeros(\n",
        "                        weight.size(0), weight.size(1), device=weight.device\n",
        "                    )\n",
        "                    mask.bernoulli_(p)\n",
        "                    mask = (\n",
        "                        mask.unsqueeze(2)\n",
        "                        .unsqueeze(3)\n",
        "                        .repeat(1, 1, mod.kernel_size[0], mod.kernel_size[1])\n",
        "                    )\n",
        "\n",
        "            # scale weights and apply mask\n",
        "            mask = mask.to(\n",
        "                torch.bool\n",
        "            )  # x.bool() is not currently supported in TorchScript\n",
        "            s = 1 / (1 - p)\n",
        "            mod.weight.data = s * weight.masked_fill(mask, 0)\n",
        "\n",
        "    module.register_forward_pre_hook(_forward_pre_hook)\n",
        "    return module\n",
        "\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention.\n",
        "\n",
        "    See \"Attention Is All You Need\" for more details.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            kdim=None,\n",
        "            vdim=None,\n",
        "            dropout=0.0,\n",
        "            bias=True,\n",
        "            add_bias_kv=False,\n",
        "            add_zero_attn=False,\n",
        "            self_attention=False,\n",
        "            encoder_decoder_attention=False,\n",
        "            q_noise=0.0,\n",
        "            qn_block_size=8,\n",
        "            has_relative_attention_bias=False,\n",
        "            num_buckets=32,\n",
        "            max_distance=128,\n",
        "            gru_rel_pos=False,\n",
        "            rescale_init=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.kdim = kdim if kdim is not None else embed_dim\n",
        "        self.vdim = vdim if vdim is not None else embed_dim\n",
        "        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_module = nn.Dropout(dropout)\n",
        "\n",
        "        self.has_relative_attention_bias = has_relative_attention_bias\n",
        "        self.num_buckets = num_buckets\n",
        "        self.max_distance = max_distance\n",
        "        if self.has_relative_attention_bias:\n",
        "            self.relative_attention_bias = nn.Embedding(num_buckets, num_heads)\n",
        "\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.q_head_dim = self.head_dim\n",
        "        self.k_head_dim = self.head_dim\n",
        "        assert (\n",
        "                self.head_dim * num_heads == self.embed_dim\n",
        "        ), \"embed_dim must be divisible by num_heads\"\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "\n",
        "        self.self_attention = self_attention\n",
        "        self.encoder_decoder_attention = encoder_decoder_attention\n",
        "\n",
        "        assert not self.self_attention or self.qkv_same_dim, (\n",
        "            \"Self-attention requires query, key and \" \"value to be of the same size\"\n",
        "        )\n",
        "\n",
        "        k_bias = True\n",
        "        if rescale_init:\n",
        "            k_bias = False\n",
        "\n",
        "        k_embed_dim = embed_dim\n",
        "        q_embed_dim = embed_dim\n",
        "\n",
        "        self.k_proj = quant_noise(\n",
        "            nn.Linear(self.kdim, k_embed_dim, bias=k_bias), q_noise, qn_block_size\n",
        "        )\n",
        "        self.v_proj = quant_noise(\n",
        "            nn.Linear(self.vdim, embed_dim, bias=bias), q_noise, qn_block_size\n",
        "        )\n",
        "        self.q_proj = quant_noise(\n",
        "            nn.Linear(embed_dim, q_embed_dim, bias=bias), q_noise, qn_block_size\n",
        "        )\n",
        "\n",
        "        self.out_proj = quant_noise(\n",
        "            nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size\n",
        "        )\n",
        "\n",
        "        if add_bias_kv:\n",
        "            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))\n",
        "            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))\n",
        "        else:\n",
        "            self.bias_k = self.bias_v = None\n",
        "\n",
        "        self.add_zero_attn = add_zero_attn\n",
        "\n",
        "        self.gru_rel_pos = gru_rel_pos\n",
        "        if self.gru_rel_pos:\n",
        "            self.grep_linear = nn.Linear(self.q_head_dim, 8)\n",
        "            self.grep_a = nn.Parameter(torch.ones(1, num_heads, 1, 1))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        if self.qkv_same_dim:\n",
        "            # Empirically observed the convergence to be much better with\n",
        "            # the scaled initialization\n",
        "            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n",
        "            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n",
        "            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n",
        "        else:\n",
        "            nn.init.xavier_uniform_(self.k_proj.weight)\n",
        "            nn.init.xavier_uniform_(self.v_proj.weight)\n",
        "            nn.init.xavier_uniform_(self.q_proj.weight)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
        "        if self.out_proj.bias is not None:\n",
        "            nn.init.constant_(self.out_proj.bias, 0.0)\n",
        "        if self.bias_k is not None:\n",
        "            nn.init.xavier_normal_(self.bias_k)\n",
        "        if self.bias_v is not None:\n",
        "            nn.init.xavier_normal_(self.bias_v)\n",
        "        if self.has_relative_attention_bias:\n",
        "            nn.init.xavier_normal_(self.relative_attention_bias.weight)\n",
        "\n",
        "    def _relative_positions_bucket(self, relative_positions, bidirectional=True):\n",
        "        num_buckets = self.num_buckets\n",
        "        max_distance = self.max_distance\n",
        "        relative_buckets = 0\n",
        "\n",
        "        if bidirectional:\n",
        "            num_buckets = num_buckets // 2\n",
        "            relative_buckets += (relative_positions > 0).to(torch.long) * num_buckets\n",
        "            relative_positions = torch.abs(relative_positions)\n",
        "        else:\n",
        "            relative_positions = -torch.min(relative_positions, torch.zeros_like(relative_positions))\n",
        "\n",
        "        max_exact = num_buckets // 2\n",
        "        is_small = relative_positions < max_exact\n",
        "\n",
        "        relative_postion_if_large = max_exact + (\n",
        "                torch.log(relative_positions.float() / max_exact)\n",
        "                / math.log(max_distance / max_exact)\n",
        "                * (num_buckets - max_exact)\n",
        "        ).to(torch.long)\n",
        "        relative_postion_if_large = torch.min(\n",
        "            relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1)\n",
        "        )\n",
        "\n",
        "        relative_buckets += torch.where(is_small, relative_positions, relative_postion_if_large)\n",
        "        return relative_buckets\n",
        "\n",
        "    def compute_bias(self, query_length, key_length):\n",
        "        context_position = torch.arange(query_length, dtype=torch.long)[:, None]\n",
        "        memory_position = torch.arange(key_length, dtype=torch.long)[None, :]\n",
        "        relative_position = memory_position - context_position\n",
        "        relative_position_bucket = self._relative_positions_bucket(\n",
        "            relative_position,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        relative_position_bucket = relative_position_bucket.to(self.relative_attention_bias.weight.device)\n",
        "        values = self.relative_attention_bias(relative_position_bucket)\n",
        "        values = values.permute([2, 0, 1])\n",
        "        return values\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            query,\n",
        "            key: Optional[Tensor],\n",
        "            value: Optional[Tensor],\n",
        "            key_padding_mask: Optional[Tensor] = None,\n",
        "            incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
        "            need_weights: bool = True,\n",
        "            static_kv: bool = False,\n",
        "            attn_mask: Optional[Tensor] = None,\n",
        "            before_softmax: bool = False,\n",
        "            need_head_weights: bool = False,\n",
        "            position_bias: Optional[Tensor] = None\n",
        "    ) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]:\n",
        "        \"\"\"Input shape: Time x Batch x Channel\n",
        "\n",
        "        Args:\n",
        "            key_padding_mask (ByteTensor, optional): mask to exclude\n",
        "                keys that are pads, of shape `(batch, src_len)`, where\n",
        "                padding elements are indicated by 1s.\n",
        "            need_weights (bool, optional): return the attention weights,\n",
        "                averaged over heads (default: False).\n",
        "            attn_mask (ByteTensor, optional): typically used to\n",
        "                implement causal attention, where the mask prevents the\n",
        "                attention from looking forward in time (default: None).\n",
        "            before_softmax (bool, optional): return the raw attention\n",
        "                weights and values before the attention softmax.\n",
        "            need_head_weights (bool, optional): return the attention\n",
        "                weights for each head. Implies *need_weights*. Default:\n",
        "                return the average attention weights over all heads.\n",
        "        \"\"\"\n",
        "        if need_head_weights:\n",
        "            need_weights = True\n",
        "\n",
        "        is_tpu = query.device.type == \"xla\"\n",
        "\n",
        "        tgt_len, bsz, embed_dim = query.size()\n",
        "        src_len = tgt_len\n",
        "        assert embed_dim == self.embed_dim\n",
        "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
        "        if key is not None:\n",
        "            src_len, key_bsz, _ = key.size()\n",
        "            if not torch.jit.is_scripting():\n",
        "                assert key_bsz == bsz\n",
        "                assert value is not None\n",
        "                assert src_len, bsz == value.shape[:2]\n",
        "\n",
        "        if self.has_relative_attention_bias and position_bias is None:\n",
        "            position_bias = self.compute_bias(tgt_len, src_len)\n",
        "            position_bias = position_bias.unsqueeze(0).repeat(bsz, 1, 1, 1).view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if (\n",
        "                not is_tpu  # don't use PyTorch version on TPUs\n",
        "                and incremental_state is None\n",
        "                and not static_kv\n",
        "                # A workaround for quantization to work. Otherwise JIT compilation\n",
        "                # treats bias in linear module as method.\n",
        "                and not torch.jit.is_scripting()\n",
        "                and self.q_head_dim == self.head_dim\n",
        "        ):\n",
        "            assert key is not None and value is not None\n",
        "            assert attn_mask is None\n",
        "\n",
        "            attn_mask_rel_pos = None\n",
        "            if position_bias is not None:\n",
        "                attn_mask_rel_pos = position_bias\n",
        "                if self.gru_rel_pos:\n",
        "                    query_layer = query.transpose(0, 1)\n",
        "                    new_x_shape = query_layer.size()[:-1] + (self.num_heads, -1)\n",
        "                    query_layer = query_layer.view(*new_x_shape)\n",
        "                    query_layer = query_layer.permute(0, 2, 1, 3)\n",
        "                    _B, _H, _L, __ = query_layer.size()\n",
        "\n",
        "                    gate_a, gate_b = torch.sigmoid(self.grep_linear(query_layer).view(\n",
        "                        _B, _H, _L, 2, 4).sum(-1, keepdim=False)).chunk(2, dim=-1)\n",
        "                    gate_a_1 = gate_a * (gate_b * self.grep_a - 1.0) + 2.0\n",
        "                    attn_mask_rel_pos = gate_a_1.view(bsz * self.num_heads, -1, 1) * position_bias\n",
        "\n",
        "                attn_mask_rel_pos = attn_mask_rel_pos.view((-1, tgt_len, tgt_len))\n",
        "            k_proj_bias = self.k_proj.bias\n",
        "            if k_proj_bias is None:\n",
        "                k_proj_bias = torch.zeros_like(self.q_proj.bias)\n",
        "\n",
        "            x, attn = F.multi_head_attention_forward(\n",
        "                query,\n",
        "                key,\n",
        "                value,\n",
        "                self.embed_dim,\n",
        "                self.num_heads,\n",
        "                torch.empty([0]),\n",
        "                torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)),\n",
        "                self.bias_k,\n",
        "                self.bias_v,\n",
        "                self.add_zero_attn,\n",
        "                self.dropout_module.p,\n",
        "                self.out_proj.weight,\n",
        "                self.out_proj.bias,\n",
        "                self.training,\n",
        "                # self.training or self.dropout_module.apply_during_inference,\n",
        "                key_padding_mask,\n",
        "                need_weights,\n",
        "                attn_mask_rel_pos,\n",
        "                use_separate_proj_weight=True,\n",
        "                q_proj_weight=self.q_proj.weight,\n",
        "                k_proj_weight=self.k_proj.weight,\n",
        "                v_proj_weight=self.v_proj.weight,\n",
        "            )\n",
        "            return x, attn, position_bias\n",
        "\n",
        "        if incremental_state is not None:\n",
        "            saved_state = self._get_input_buffer(incremental_state)\n",
        "            if saved_state is not None and \"prev_key\" in saved_state:\n",
        "                # previous time steps are cached - no need to recompute\n",
        "                # key and value if they are static\n",
        "                if static_kv:\n",
        "                    assert self.encoder_decoder_attention and not self.self_attention\n",
        "                    key = value = None\n",
        "        else:\n",
        "            saved_state = None\n",
        "\n",
        "        if self.self_attention:\n",
        "            q = self.q_proj(query)\n",
        "            k = self.k_proj(query)\n",
        "            v = self.v_proj(query)\n",
        "        elif self.encoder_decoder_attention:\n",
        "            # encoder-decoder attention\n",
        "            q = self.q_proj(query)\n",
        "            if key is None:\n",
        "                assert value is None\n",
        "                k = v = None\n",
        "            else:\n",
        "                k = self.k_proj(key)\n",
        "                v = self.v_proj(key)\n",
        "\n",
        "        else:\n",
        "            assert key is not None and value is not None\n",
        "            q = self.q_proj(query)\n",
        "            k = self.k_proj(key)\n",
        "            v = self.v_proj(value)\n",
        "        q *= self.scaling\n",
        "\n",
        "        if self.bias_k is not None:\n",
        "            assert self.bias_v is not None\n",
        "            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n",
        "            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n",
        "            if attn_mask is not None:\n",
        "                attn_mask = torch.cat(\n",
        "                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n",
        "                )\n",
        "            if key_padding_mask is not None:\n",
        "                key_padding_mask = torch.cat(\n",
        "                    [\n",
        "                        key_padding_mask,\n",
        "                        key_padding_mask.new_zeros(key_padding_mask.size(0), 1),\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "\n",
        "        q = (\n",
        "            q.contiguous()\n",
        "                .view(tgt_len, bsz * self.num_heads, self.q_head_dim)\n",
        "                .transpose(0, 1)\n",
        "        )\n",
        "        if k is not None:\n",
        "            k = (\n",
        "                k.contiguous()\n",
        "                    .view(-1, bsz * self.num_heads, self.k_head_dim)\n",
        "                    .transpose(0, 1)\n",
        "            )\n",
        "        if v is not None:\n",
        "            v = (\n",
        "                v.contiguous()\n",
        "                    .view(-1, bsz * self.num_heads, self.head_dim)\n",
        "                    .transpose(0, 1)\n",
        "            )\n",
        "\n",
        "        if saved_state is not None:\n",
        "            # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n",
        "            if \"prev_key\" in saved_state:\n",
        "                _prev_key = saved_state[\"prev_key\"]\n",
        "                assert _prev_key is not None\n",
        "                prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "                if static_kv:\n",
        "                    k = prev_key\n",
        "                else:\n",
        "                    assert k is not None\n",
        "                    k = torch.cat([prev_key, k], dim=1)\n",
        "                src_len = k.size(1)\n",
        "            if \"prev_value\" in saved_state:\n",
        "                _prev_value = saved_state[\"prev_value\"]\n",
        "                assert _prev_value is not None\n",
        "                prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "                if static_kv:\n",
        "                    v = prev_value\n",
        "                else:\n",
        "                    assert v is not None\n",
        "                    v = torch.cat([prev_value, v], dim=1)\n",
        "            prev_key_padding_mask: Optional[Tensor] = None\n",
        "            if \"prev_key_padding_mask\" in saved_state:\n",
        "                prev_key_padding_mask = saved_state[\"prev_key_padding_mask\"]\n",
        "            assert k is not None and v is not None\n",
        "            key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(\n",
        "                key_padding_mask=key_padding_mask,\n",
        "                prev_key_padding_mask=prev_key_padding_mask,\n",
        "                batch_size=bsz,\n",
        "                src_len=k.size(1),\n",
        "                static_kv=static_kv,\n",
        "            )\n",
        "\n",
        "            saved_state[\"prev_key\"] = k.view(bsz, self.num_heads, -1, self.head_dim)\n",
        "            saved_state[\"prev_value\"] = v.view(bsz, self.num_heads, -1, self.head_dim)\n",
        "            saved_state[\"prev_key_padding_mask\"] = key_padding_mask\n",
        "            # In this branch incremental_state is never None\n",
        "            assert incremental_state is not None\n",
        "            incremental_state = self._set_input_buffer(incremental_state, saved_state)\n",
        "        assert k is not None\n",
        "        assert k.size(1) == src_len\n",
        "\n",
        "        # This is part of a workaround to get around fork/join parallelism\n",
        "        # not supporting Optional types.\n",
        "        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n",
        "            key_padding_mask = None\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            assert key_padding_mask.size(0) == bsz\n",
        "            assert key_padding_mask.size(1) == src_len\n",
        "\n",
        "        if self.add_zero_attn:\n",
        "            assert v is not None\n",
        "            src_len += 1\n",
        "            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n",
        "            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n",
        "            if attn_mask is not None:\n",
        "                attn_mask = torch.cat(\n",
        "                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n",
        "                )\n",
        "            if key_padding_mask is not None:\n",
        "                key_padding_mask = torch.cat(\n",
        "                    [\n",
        "                        key_padding_mask,\n",
        "                        torch.zeros(key_padding_mask.size(0), 1).type_as(\n",
        "                            key_padding_mask\n",
        "                        ),\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "\n",
        "        attn_weights = torch.bmm(q, k.transpose(1, 2))\n",
        "        attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n",
        "\n",
        "        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_mask = attn_mask.unsqueeze(0)\n",
        "            attn_weights += attn_mask\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            # don't attend to padding symbols\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            if not is_tpu:\n",
        "                attn_weights = attn_weights.masked_fill(\n",
        "                    key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool),\n",
        "                    float(\"-inf\"),\n",
        "                )\n",
        "            else:\n",
        "                attn_weights = attn_weights.transpose(0, 2)\n",
        "                attn_weights = attn_weights.masked_fill(key_padding_mask, float(\"-inf\"))\n",
        "                attn_weights = attn_weights.transpose(0, 2)\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if before_softmax:\n",
        "            return attn_weights, v, position_bias\n",
        "\n",
        "        if position_bias is not None:\n",
        "            if self.gru_rel_pos == 1:\n",
        "                query_layer = q.view(bsz, self.num_heads, tgt_len, self.q_head_dim)\n",
        "                _B, _H, _L, __ = query_layer.size()\n",
        "                gate_a, gate_b = torch.sigmoid(self.grep_linear(query_layer).view(\n",
        "                    _B, _H, _L, 2, 4).sum(-1, keepdim=False)).chunk(2, dim=-1)\n",
        "                gate_a_1 = gate_a * (gate_b * self.grep_a - 1.0) + 2.0\n",
        "                position_bias = gate_a_1.view(bsz * self.num_heads, -1, 1) * position_bias\n",
        "\n",
        "            position_bias = position_bias.view(attn_weights.size())\n",
        "\n",
        "            attn_weights = attn_weights + position_bias\n",
        "\n",
        "        attn_weights_float = F.softmax(\n",
        "            attn_weights, dim=-1\n",
        "        )\n",
        "        attn_weights = attn_weights_float.type_as(attn_weights)\n",
        "        attn_probs = self.dropout_module(attn_weights)\n",
        "\n",
        "        assert v is not None\n",
        "        attn = torch.bmm(attn_probs, v)\n",
        "        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n",
        "        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
        "        attn = self.out_proj(attn)\n",
        "        attn_weights: Optional[Tensor] = None\n",
        "        if need_weights:\n",
        "            attn_weights = attn_weights_float.view(\n",
        "                bsz, self.num_heads, tgt_len, src_len\n",
        "            ).transpose(1, 0)\n",
        "            if not need_head_weights:\n",
        "                # average attention weights over heads\n",
        "                attn_weights = attn_weights.mean(dim=0)\n",
        "\n",
        "        return attn, attn_weights, position_bias\n",
        "\n",
        "    @staticmethod\n",
        "    def _append_prev_key_padding_mask(\n",
        "            key_padding_mask: Optional[Tensor],\n",
        "            prev_key_padding_mask: Optional[Tensor],\n",
        "            batch_size: int,\n",
        "            src_len: int,\n",
        "            static_kv: bool,\n",
        "    ) -> Optional[Tensor]:\n",
        "        # saved key padding masks have shape (bsz, seq_len)\n",
        "        if prev_key_padding_mask is not None and static_kv:\n",
        "            new_key_padding_mask = prev_key_padding_mask\n",
        "        elif prev_key_padding_mask is not None and key_padding_mask is not None:\n",
        "            new_key_padding_mask = torch.cat(\n",
        "                [prev_key_padding_mask.float(), key_padding_mask.float()], dim=1\n",
        "            )\n",
        "        # During incremental decoding, as the padding token enters and\n",
        "        # leaves the frame, there will be a time when prev or current\n",
        "        # is None\n",
        "        elif prev_key_padding_mask is not None:\n",
        "            if src_len > prev_key_padding_mask.size(1):\n",
        "                filler = torch.zeros(\n",
        "                    (batch_size, src_len - prev_key_padding_mask.size(1)),\n",
        "                    device=prev_key_padding_mask.device,\n",
        "                )\n",
        "                new_key_padding_mask = torch.cat(\n",
        "                    [prev_key_padding_mask.float(), filler.float()], dim=1\n",
        "                )\n",
        "            else:\n",
        "                new_key_padding_mask = prev_key_padding_mask.float()\n",
        "        elif key_padding_mask is not None:\n",
        "            if src_len > key_padding_mask.size(1):\n",
        "                filler = torch.zeros(\n",
        "                    (batch_size, src_len - key_padding_mask.size(1)),\n",
        "                    device=key_padding_mask.device,\n",
        "                )\n",
        "                new_key_padding_mask = torch.cat(\n",
        "                    [filler.float(), key_padding_mask.float()], dim=1\n",
        "                )\n",
        "            else:\n",
        "                new_key_padding_mask = key_padding_mask.float()\n",
        "        else:\n",
        "            new_key_padding_mask = prev_key_padding_mask\n",
        "        return new_key_padding_mask\n",
        "\n",
        "    def _get_input_buffer(\n",
        "            self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]\n",
        "    ) -> Dict[str, Optional[Tensor]]:\n",
        "        result = self.get_incremental_state(incremental_state, \"attn_state\")\n",
        "        if result is not None:\n",
        "            return result\n",
        "        else:\n",
        "            empty_result: Dict[str, Optional[Tensor]] = {}\n",
        "            return empty_result\n",
        "\n",
        "    def _set_input_buffer(\n",
        "            self,\n",
        "            incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n",
        "            buffer: Dict[str, Optional[Tensor]],\n",
        "    ):\n",
        "        return self.set_incremental_state(incremental_state, \"attn_state\", buffer)\n",
        "\n",
        "    def apply_sparse_mask(self, attn_weights, tgt_len: int, src_len: int, bsz: int):\n",
        "        return attn_weights"
      ],
      "metadata": {
        "id": "LdueUxsoQBjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------\n",
        "# WavLM: Large-Scale Self-Supervised  Pre-training  for Full Stack Speech Processing (https://arxiv.org/abs/2110.13900.pdf)\n",
        "# Github source: https://github.com/microsoft/unilm/tree/master/wavlm\n",
        "# Copyright (c) 2021 Microsoft\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# Based on fairseq code bases\n",
        "# https://github.com/pytorch/fairseq\n",
        "# --------------------------------------------------------\n",
        "\n",
        "import math\n",
        "import warnings\n",
        "from typing import Dict, Optional, Tuple\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TransposeLast(nn.Module):\n",
        "    def __init__(self, deconstruct_idx=None):\n",
        "        super().__init__()\n",
        "        self.deconstruct_idx = deconstruct_idx\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.deconstruct_idx is not None:\n",
        "            x = x[self.deconstruct_idx]\n",
        "        return x.transpose(-2, -1)\n",
        "\n",
        "\n",
        "class Fp32LayerNorm(nn.LayerNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = F.layer_norm(\n",
        "            input.float(),\n",
        "            self.normalized_shape,\n",
        "            self.weight.float() if self.weight is not None else None,\n",
        "            self.bias.float() if self.bias is not None else None,\n",
        "            self.eps,\n",
        "        )\n",
        "        return output.type_as(input)\n",
        "\n",
        "\n",
        "class Fp32GroupNorm(nn.GroupNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = F.group_norm(\n",
        "            input.float(),\n",
        "            self.num_groups,\n",
        "            self.weight.float() if self.weight is not None else None,\n",
        "            self.bias.float() if self.bias is not None else None,\n",
        "            self.eps,\n",
        "        )\n",
        "        return output.type_as(input)\n",
        "\n",
        "\n",
        "class GradMultiply(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, scale):\n",
        "        ctx.scale = scale\n",
        "        res = x.new(x)\n",
        "        return res\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad):\n",
        "        return grad * ctx.scale, None\n",
        "\n",
        "\n",
        "class SamePad(nn.Module):\n",
        "    def __init__(self, kernel_size, causal=False):\n",
        "        super().__init__()\n",
        "        if causal:\n",
        "            self.remove = kernel_size - 1\n",
        "        else:\n",
        "            self.remove = 1 if kernel_size % 2 == 0 else 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.remove > 0:\n",
        "            x = x[:, :, : -self.remove]\n",
        "        return x\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    \"\"\"Swish function\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Construct an MultiHeadedAttention object.\"\"\"\n",
        "        super(Swish, self).__init__()\n",
        "        self.act = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.act(x)\n",
        "\n",
        "\n",
        "class GLU_Linear(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, glu_type=\"sigmoid\", bias_in_glu=True):\n",
        "        super(GLU_Linear, self).__init__()\n",
        "\n",
        "        self.glu_type = glu_type\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        if glu_type == \"sigmoid\":\n",
        "            self.glu_act = torch.nn.Sigmoid()\n",
        "        elif glu_type == \"swish\":\n",
        "            self.glu_act = Swish()\n",
        "        elif glu_type == \"relu\":\n",
        "            self.glu_act = torch.nn.ReLU()\n",
        "        elif glu_type == \"gelu\":\n",
        "            self.glu_act = torch.nn.GELU()\n",
        "\n",
        "        if bias_in_glu:\n",
        "            self.linear = nn.Linear(input_dim, output_dim * 2, True)\n",
        "        else:\n",
        "            self.linear = nn.Linear(input_dim, output_dim * 2, False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # to be consistent with GLU_Linear, we assume the input always has the #channel (#dim) in the last dimension of the tensor, so need to switch the dimension first for 1D-Conv case\n",
        "        x = self.linear(x)\n",
        "\n",
        "        if self.glu_type == \"bilinear\":\n",
        "            x = (x[:, :, 0:self.output_dim] * x[:, :, self.output_dim:self.output_dim * 2])\n",
        "        else:\n",
        "            x = (x[:, :, 0:self.output_dim] * self.glu_act(x[:, :, self.output_dim:self.output_dim * 2]))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def gelu_accurate(x):\n",
        "    if not hasattr(gelu_accurate, \"_a\"):\n",
        "        gelu_accurate._a = math.sqrt(2 / math.pi)\n",
        "    return (\n",
        "        0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    )\n",
        "\n",
        "\n",
        "def gelu(x: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.nn.functional.gelu(x.float()).type_as(x)\n",
        "\n",
        "\n",
        "def get_activation_fn(activation: str):\n",
        "    \"\"\"Returns the activation function corresponding to `activation`\"\"\"\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    elif activation == \"gelu\":\n",
        "        return gelu\n",
        "    elif activation == \"gelu_fast\":\n",
        "        warnings.warn(\n",
        "            \"--activation-fn=gelu_fast has been renamed to gelu_accurate\"\n",
        "        )\n",
        "        return gelu_accurate\n",
        "    elif activation == \"gelu_accurate\":\n",
        "        return gelu_accurate\n",
        "    elif activation == \"tanh\":\n",
        "        return torch.tanh\n",
        "    elif activation == \"linear\":\n",
        "        return lambda x: x\n",
        "    elif activation == \"glu\":\n",
        "        return lambda x: x\n",
        "    else:\n",
        "        raise RuntimeError(\"--activation-fn {} not supported\".format(activation))\n",
        "\n",
        "\n",
        "def init_bert_params(module):\n",
        "    \"\"\"\n",
        "    Initialize the weights specific to the BERT Model.\n",
        "    This overrides the default initializations depending on the specified arguments.\n",
        "        1. If normal_init_linear_weights is set then weights of linear\n",
        "           layer will be initialized using the normal distribution and\n",
        "           bais will be set to the specified value.\n",
        "        2. If normal_init_embed_weights is set then weights of embedding\n",
        "           layer will be initialized using the normal distribution.\n",
        "        3. If normal_init_proj_weights is set then weights of\n",
        "           in_project_weight for MultiHeadAttention initialized using\n",
        "           the normal distribution (to be validated).\n",
        "    \"\"\"\n",
        "\n",
        "    def normal_(data):\n",
        "        # with FSDP, module params will be on CUDA, so we cast them back to CPU\n",
        "        # so that the RNG is consistent with and without FSDP\n",
        "        data.copy_(\n",
        "            data.cpu().normal_(mean=0.0, std=0.02).to(data.device)\n",
        "        )\n",
        "\n",
        "    if isinstance(module, nn.Linear):\n",
        "        normal_(module.weight.data)\n",
        "        if module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "    if isinstance(module, nn.Embedding):\n",
        "        normal_(module.weight.data)\n",
        "        if module.padding_idx is not None:\n",
        "            module.weight.data[module.padding_idx].zero_()\n",
        "    if isinstance(module, MultiheadAttention):\n",
        "        normal_(module.q_proj.weight.data)\n",
        "        normal_(module.k_proj.weight.data)\n",
        "        normal_(module.v_proj.weight.data)\n",
        "\n",
        "\n",
        "def quant_noise(module, p, block_size):\n",
        "    \"\"\"\n",
        "    Wraps modules and applies quantization noise to the weights for\n",
        "    subsequent quantization with Iterative Product Quantization as\n",
        "    described in \"Training with Quantization Noise for Extreme Model Compression\"\n",
        "\n",
        "    Args:\n",
        "        - module: nn.Module\n",
        "        - p: amount of Quantization Noise\n",
        "        - block_size: size of the blocks for subsequent quantization with iPQ\n",
        "\n",
        "    Remarks:\n",
        "        - Module weights must have the right sizes wrt the block size\n",
        "        - Only Linear, Embedding and Conv2d modules are supported for the moment\n",
        "        - For more detail on how to quantize by blocks with convolutional weights,\n",
        "          see \"And the Bit Goes Down: Revisiting the Quantization of Neural Networks\"\n",
        "        - We implement the simplest form of noise here as stated in the paper\n",
        "          which consists in randomly dropping blocks\n",
        "    \"\"\"\n",
        "\n",
        "    # if no quantization noise, don't register hook\n",
        "    if p <= 0:\n",
        "        return module\n",
        "\n",
        "    # supported modules\n",
        "    assert isinstance(module, (nn.Linear, nn.Embedding, nn.Conv2d))\n",
        "\n",
        "    # test whether module.weight has the right sizes wrt block_size\n",
        "    is_conv = module.weight.ndim == 4\n",
        "\n",
        "    # 2D matrix\n",
        "    if not is_conv:\n",
        "        assert (\n",
        "            module.weight.size(1) % block_size == 0\n",
        "        ), \"Input features must be a multiple of block sizes\"\n",
        "\n",
        "    # 4D matrix\n",
        "    else:\n",
        "        # 1x1 convolutions\n",
        "        if module.kernel_size == (1, 1):\n",
        "            assert (\n",
        "                module.in_channels % block_size == 0\n",
        "            ), \"Input channels must be a multiple of block sizes\"\n",
        "        # regular convolutions\n",
        "        else:\n",
        "            k = module.kernel_size[0] * module.kernel_size[1]\n",
        "            assert k % block_size == 0, \"Kernel size must be a multiple of block size\"\n",
        "\n",
        "    def _forward_pre_hook(mod, input):\n",
        "        # no noise for evaluation\n",
        "        if mod.training:\n",
        "            if not is_conv:\n",
        "                # gather weight and sizes\n",
        "                weight = mod.weight\n",
        "                in_features = weight.size(1)\n",
        "                out_features = weight.size(0)\n",
        "\n",
        "                # split weight matrix into blocks and randomly drop selected blocks\n",
        "                mask = torch.zeros(\n",
        "                    in_features // block_size * out_features, device=weight.device\n",
        "                )\n",
        "                mask.bernoulli_(p)\n",
        "                mask = mask.repeat_interleave(block_size, -1).view(-1, in_features)\n",
        "\n",
        "            else:\n",
        "                # gather weight and sizes\n",
        "                weight = mod.weight\n",
        "                in_channels = mod.in_channels\n",
        "                out_channels = mod.out_channels\n",
        "\n",
        "                # split weight matrix into blocks and randomly drop selected blocks\n",
        "                if mod.kernel_size == (1, 1):\n",
        "                    mask = torch.zeros(\n",
        "                        int(in_channels // block_size * out_channels),\n",
        "                        device=weight.device,\n",
        "                    )\n",
        "                    mask.bernoulli_(p)\n",
        "                    mask = mask.repeat_interleave(block_size, -1).view(-1, in_channels)\n",
        "                else:\n",
        "                    mask = torch.zeros(\n",
        "                        weight.size(0), weight.size(1), device=weight.device\n",
        "                    )\n",
        "                    mask.bernoulli_(p)\n",
        "                    mask = (\n",
        "                        mask.unsqueeze(2)\n",
        "                        .unsqueeze(3)\n",
        "                        .repeat(1, 1, mod.kernel_size[0], mod.kernel_size[1])\n",
        "                    )\n",
        "\n",
        "            # scale weights and apply mask\n",
        "            mask = mask.to(\n",
        "                torch.bool\n",
        "            )  # x.bool() is not currently supported in TorchScript\n",
        "            s = 1 / (1 - p)\n",
        "            mod.weight.data = s * weight.masked_fill(mask, 0)\n",
        "\n",
        "    module.register_forward_pre_hook(_forward_pre_hook)\n",
        "    return module\n",
        "\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention.\n",
        "\n",
        "    See \"Attention Is All You Need\" for more details.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            kdim=None,\n",
        "            vdim=None,\n",
        "            dropout=0.0,\n",
        "            bias=True,\n",
        "            add_bias_kv=False,\n",
        "            add_zero_attn=False,\n",
        "            self_attention=False,\n",
        "            encoder_decoder_attention=False,\n",
        "            q_noise=0.0,\n",
        "            qn_block_size=8,\n",
        "            has_relative_attention_bias=False,\n",
        "            num_buckets=32,\n",
        "            max_distance=128,\n",
        "            gru_rel_pos=False,\n",
        "            rescale_init=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.kdim = kdim if kdim is not None else embed_dim\n",
        "        self.vdim = vdim if vdim is not None else embed_dim\n",
        "        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_module = nn.Dropout(dropout)\n",
        "\n",
        "        self.has_relative_attention_bias = has_relative_attention_bias\n",
        "        self.num_buckets = num_buckets\n",
        "        self.max_distance = max_distance\n",
        "        if self.has_relative_attention_bias:\n",
        "            self.relative_attention_bias = nn.Embedding(num_buckets, num_heads)\n",
        "\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.q_head_dim = self.head_dim\n",
        "        self.k_head_dim = self.head_dim\n",
        "        assert (\n",
        "                self.head_dim * num_heads == self.embed_dim\n",
        "        ), \"embed_dim must be divisible by num_heads\"\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "\n",
        "        self.self_attention = self_attention\n",
        "        self.encoder_decoder_attention = encoder_decoder_attention\n",
        "\n",
        "        assert not self.self_attention or self.qkv_same_dim, (\n",
        "            \"Self-attention requires query, key and \" \"value to be of the same size\"\n",
        "        )\n",
        "\n",
        "        k_bias = True\n",
        "        if rescale_init:\n",
        "            k_bias = False\n",
        "\n",
        "        k_embed_dim = embed_dim\n",
        "        q_embed_dim = embed_dim\n",
        "\n",
        "        self.k_proj = quant_noise(\n",
        "            nn.Linear(self.kdim, k_embed_dim, bias=k_bias), q_noise, qn_block_size\n",
        "        )\n",
        "        self.v_proj = quant_noise(\n",
        "            nn.Linear(self.vdim, embed_dim, bias=bias), q_noise, qn_block_size\n",
        "        )\n",
        "        self.q_proj = quant_noise(\n",
        "            nn.Linear(embed_dim, q_embed_dim, bias=bias), q_noise, qn_block_size\n",
        "        )\n",
        "\n",
        "        self.out_proj = quant_noise(\n",
        "            nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size\n",
        "        )\n",
        "\n",
        "        if add_bias_kv:\n",
        "            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))\n",
        "            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))\n",
        "        else:\n",
        "            self.bias_k = self.bias_v = None\n",
        "\n",
        "        self.add_zero_attn = add_zero_attn\n",
        "\n",
        "        self.gru_rel_pos = gru_rel_pos\n",
        "        if self.gru_rel_pos:\n",
        "            self.grep_linear = nn.Linear(self.q_head_dim, 8)\n",
        "            self.grep_a = nn.Parameter(torch.ones(1, num_heads, 1, 1))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        if self.qkv_same_dim:\n",
        "            # Empirically observed the convergence to be much better with\n",
        "            # the scaled initialization\n",
        "            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n",
        "            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n",
        "            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n",
        "        else:\n",
        "            nn.init.xavier_uniform_(self.k_proj.weight)\n",
        "            nn.init.xavier_uniform_(self.v_proj.weight)\n",
        "            nn.init.xavier_uniform_(self.q_proj.weight)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
        "        if self.out_proj.bias is not None:\n",
        "            nn.init.constant_(self.out_proj.bias, 0.0)\n",
        "        if self.bias_k is not None:\n",
        "            nn.init.xavier_normal_(self.bias_k)\n",
        "        if self.bias_v is not None:\n",
        "            nn.init.xavier_normal_(self.bias_v)\n",
        "        if self.has_relative_attention_bias:\n",
        "            nn.init.xavier_normal_(self.relative_attention_bias.weight)\n",
        "\n",
        "    def _relative_positions_bucket(self, relative_positions, bidirectional=True):\n",
        "        num_buckets = self.num_buckets\n",
        "        max_distance = self.max_distance\n",
        "        relative_buckets = 0\n",
        "\n",
        "        if bidirectional:\n",
        "            num_buckets = num_buckets // 2\n",
        "            relative_buckets += (relative_positions > 0).to(torch.long) * num_buckets\n",
        "            relative_positions = torch.abs(relative_positions)\n",
        "        else:\n",
        "            relative_positions = -torch.min(relative_positions, torch.zeros_like(relative_positions))\n",
        "\n",
        "        max_exact = num_buckets // 2\n",
        "        is_small = relative_positions < max_exact\n",
        "\n",
        "        relative_postion_if_large = max_exact + (\n",
        "                torch.log(relative_positions.float() / max_exact)\n",
        "                / math.log(max_distance / max_exact)\n",
        "                * (num_buckets - max_exact)\n",
        "        ).to(torch.long)\n",
        "        relative_postion_if_large = torch.min(\n",
        "            relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1)\n",
        "        )\n",
        "\n",
        "        relative_buckets += torch.where(is_small, relative_positions, relative_postion_if_large)\n",
        "        return relative_buckets\n",
        "\n",
        "    def compute_bias(self, query_length, key_length):\n",
        "        context_position = torch.arange(query_length, dtype=torch.long)[:, None]\n",
        "        memory_position = torch.arange(key_length, dtype=torch.long)[None, :]\n",
        "        relative_position = memory_position - context_position\n",
        "        relative_position_bucket = self._relative_positions_bucket(\n",
        "            relative_position,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        relative_position_bucket = relative_position_bucket.to(self.relative_attention_bias.weight.device)\n",
        "        values = self.relative_attention_bias(relative_position_bucket)\n",
        "        values = values.permute([2, 0, 1])\n",
        "        return values\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            query,\n",
        "            key: Optional[Tensor],\n",
        "            value: Optional[Tensor],\n",
        "            key_padding_mask: Optional[Tensor] = None,\n",
        "            incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
        "            need_weights: bool = True,\n",
        "            static_kv: bool = False,\n",
        "            attn_mask: Optional[Tensor] = None,\n",
        "            before_softmax: bool = False,\n",
        "            need_head_weights: bool = False,\n",
        "            position_bias: Optional[Tensor] = None\n",
        "    ) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]:\n",
        "        \"\"\"Input shape: Time x Batch x Channel\n",
        "\n",
        "        Args:\n",
        "            key_padding_mask (ByteTensor, optional): mask to exclude\n",
        "                keys that are pads, of shape `(batch, src_len)`, where\n",
        "                padding elements are indicated by 1s.\n",
        "            need_weights (bool, optional): return the attention weights,\n",
        "                averaged over heads (default: False).\n",
        "            attn_mask (ByteTensor, optional): typically used to\n",
        "                implement causal attention, where the mask prevents the\n",
        "                attention from looking forward in time (default: None).\n",
        "            before_softmax (bool, optional): return the raw attention\n",
        "                weights and values before the attention softmax.\n",
        "            need_head_weights (bool, optional): return the attention\n",
        "                weights for each head. Implies *need_weights*. Default:\n",
        "                return the average attention weights over all heads.\n",
        "        \"\"\"\n",
        "        if need_head_weights:\n",
        "            need_weights = True\n",
        "\n",
        "        is_tpu = query.device.type == \"xla\"\n",
        "\n",
        "        tgt_len, bsz, embed_dim = query.size()\n",
        "        src_len = tgt_len\n",
        "        assert embed_dim == self.embed_dim\n",
        "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
        "        if key is not None:\n",
        "            src_len, key_bsz, _ = key.size()\n",
        "            if not torch.jit.is_scripting():\n",
        "                assert key_bsz == bsz\n",
        "                assert value is not None\n",
        "                assert src_len, bsz == value.shape[:2]\n",
        "\n",
        "        if self.has_relative_attention_bias and position_bias is None:\n",
        "            position_bias = self.compute_bias(tgt_len, src_len)\n",
        "            position_bias = position_bias.unsqueeze(0).repeat(bsz, 1, 1, 1).view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if (\n",
        "                not is_tpu  # don't use PyTorch version on TPUs\n",
        "                and incremental_state is None\n",
        "                and not static_kv\n",
        "                # A workaround for quantization to work. Otherwise JIT compilation\n",
        "                # treats bias in linear module as method.\n",
        "                and not torch.jit.is_scripting()\n",
        "                and self.q_head_dim == self.head_dim\n",
        "        ):\n",
        "            assert key is not None and value is not None\n",
        "            assert attn_mask is None\n",
        "\n",
        "            attn_mask_rel_pos = None\n",
        "            if position_bias is not None:\n",
        "                attn_mask_rel_pos = position_bias\n",
        "                if self.gru_rel_pos:\n",
        "                    query_layer = query.transpose(0, 1)\n",
        "                    new_x_shape = query_layer.size()[:-1] + (self.num_heads, -1)\n",
        "                    query_layer = query_layer.view(*new_x_shape)\n",
        "                    query_layer = query_layer.permute(0, 2, 1, 3)\n",
        "                    _B, _H, _L, __ = query_layer.size()\n",
        "\n",
        "                    gate_a, gate_b = torch.sigmoid(self.grep_linear(query_layer).view(\n",
        "                        _B, _H, _L, 2, 4).sum(-1, keepdim=False)).chunk(2, dim=-1)\n",
        "                    gate_a_1 = gate_a * (gate_b * self.grep_a - 1.0) + 2.0\n",
        "                    attn_mask_rel_pos = gate_a_1.view(bsz * self.num_heads, -1, 1) * position_bias\n",
        "\n",
        "                attn_mask_rel_pos = attn_mask_rel_pos.view((-1, tgt_len, tgt_len))\n",
        "            k_proj_bias = self.k_proj.bias\n",
        "            if k_proj_bias is None:\n",
        "                k_proj_bias = torch.zeros_like(self.q_proj.bias)\n",
        "\n",
        "            x, attn = F.multi_head_attention_forward(\n",
        "                query,\n",
        "                key,\n",
        "                value,\n",
        "                self.embed_dim,\n",
        "                self.num_heads,\n",
        "                torch.empty([0]),\n",
        "                torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)),\n",
        "                self.bias_k,\n",
        "                self.bias_v,\n",
        "                self.add_zero_attn,\n",
        "                self.dropout_module.p,\n",
        "                self.out_proj.weight,\n",
        "                self.out_proj.bias,\n",
        "                self.training,\n",
        "                # self.training or self.dropout_module.apply_during_inference,\n",
        "                key_padding_mask,\n",
        "                need_weights,\n",
        "                attn_mask_rel_pos,\n",
        "                use_separate_proj_weight=True,\n",
        "                q_proj_weight=self.q_proj.weight,\n",
        "                k_proj_weight=self.k_proj.weight,\n",
        "                v_proj_weight=self.v_proj.weight,\n",
        "            )\n",
        "            return x, attn, position_bias\n",
        "\n",
        "        if incremental_state is not None:\n",
        "            saved_state = self._get_input_buffer(incremental_state)\n",
        "            if saved_state is not None and \"prev_key\" in saved_state:\n",
        "                # previous time steps are cached - no need to recompute\n",
        "                # key and value if they are static\n",
        "                if static_kv:\n",
        "                    assert self.encoder_decoder_attention and not self.self_attention\n",
        "                    key = value = None\n",
        "        else:\n",
        "            saved_state = None\n",
        "\n",
        "        if self.self_attention:\n",
        "            q = self.q_proj(query)\n",
        "            k = self.k_proj(query)\n",
        "            v = self.v_proj(query)\n",
        "        elif self.encoder_decoder_attention:\n",
        "            # encoder-decoder attention\n",
        "            q = self.q_proj(query)\n",
        "            if key is None:\n",
        "                assert value is None\n",
        "                k = v = None\n",
        "            else:\n",
        "                k = self.k_proj(key)\n",
        "                v = self.v_proj(key)\n",
        "\n",
        "        else:\n",
        "            assert key is not None and value is not None\n",
        "            q = self.q_proj(query)\n",
        "            k = self.k_proj(key)\n",
        "            v = self.v_proj(value)\n",
        "        q *= self.scaling\n",
        "\n",
        "        if self.bias_k is not None:\n",
        "            assert self.bias_v is not None\n",
        "            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n",
        "            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n",
        "            if attn_mask is not None:\n",
        "                attn_mask = torch.cat(\n",
        "                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n",
        "                )\n",
        "            if key_padding_mask is not None:\n",
        "                key_padding_mask = torch.cat(\n",
        "                    [\n",
        "                        key_padding_mask,\n",
        "                        key_padding_mask.new_zeros(key_padding_mask.size(0), 1),\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "\n",
        "        q = (\n",
        "            q.contiguous()\n",
        "                .view(tgt_len, bsz * self.num_heads, self.q_head_dim)\n",
        "                .transpose(0, 1)\n",
        "        )\n",
        "        if k is not None:\n",
        "            k = (\n",
        "                k.contiguous()\n",
        "                    .view(-1, bsz * self.num_heads, self.k_head_dim)\n",
        "                    .transpose(0, 1)\n",
        "            )\n",
        "        if v is not None:\n",
        "            v = (\n",
        "                v.contiguous()\n",
        "                    .view(-1, bsz * self.num_heads, self.head_dim)\n",
        "                    .transpose(0, 1)\n",
        "            )\n",
        "\n",
        "        if saved_state is not None:\n",
        "            # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n",
        "            if \"prev_key\" in saved_state:\n",
        "                _prev_key = saved_state[\"prev_key\"]\n",
        "                assert _prev_key is not None\n",
        "                prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "                if static_kv:\n",
        "                    k = prev_key\n",
        "                else:\n",
        "                    assert k is not None\n",
        "                    k = torch.cat([prev_key, k], dim=1)\n",
        "                src_len = k.size(1)\n",
        "            if \"prev_value\" in saved_state:\n",
        "                _prev_value = saved_state[\"prev_value\"]\n",
        "                assert _prev_value is not None\n",
        "                prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)\n",
        "                if static_kv:\n",
        "                    v = prev_value\n",
        "                else:\n",
        "                    assert v is not None\n",
        "                    v = torch.cat([prev_value, v], dim=1)\n",
        "            prev_key_padding_mask: Optional[Tensor] = None\n",
        "            if \"prev_key_padding_mask\" in saved_state:\n",
        "                prev_key_padding_mask = saved_state[\"prev_key_padding_mask\"]\n",
        "            assert k is not None and v is not None\n",
        "            key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(\n",
        "                key_padding_mask=key_padding_mask,\n",
        "                prev_key_padding_mask=prev_key_padding_mask,\n",
        "                batch_size=bsz,\n",
        "                src_len=k.size(1),\n",
        "                static_kv=static_kv,\n",
        "            )\n",
        "\n",
        "            saved_state[\"prev_key\"] = k.view(bsz, self.num_heads, -1, self.head_dim)\n",
        "            saved_state[\"prev_value\"] = v.view(bsz, self.num_heads, -1, self.head_dim)\n",
        "            saved_state[\"prev_key_padding_mask\"] = key_padding_mask\n",
        "            # In this branch incremental_state is never None\n",
        "            assert incremental_state is not None\n",
        "            incremental_state = self._set_input_buffer(incremental_state, saved_state)\n",
        "        assert k is not None\n",
        "        assert k.size(1) == src_len\n",
        "\n",
        "        # This is part of a workaround to get around fork/join parallelism\n",
        "        # not supporting Optional types.\n",
        "        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n",
        "            key_padding_mask = None\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            assert key_padding_mask.size(0) == bsz\n",
        "            assert key_padding_mask.size(1) == src_len\n",
        "\n",
        "        if self.add_zero_attn:\n",
        "            assert v is not None\n",
        "            src_len += 1\n",
        "            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n",
        "            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n",
        "            if attn_mask is not None:\n",
        "                attn_mask = torch.cat(\n",
        "                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n",
        "                )\n",
        "            if key_padding_mask is not None:\n",
        "                key_padding_mask = torch.cat(\n",
        "                    [\n",
        "                        key_padding_mask,\n",
        "                        torch.zeros(key_padding_mask.size(0), 1).type_as(\n",
        "                            key_padding_mask\n",
        "                        ),\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "\n",
        "        attn_weights = torch.bmm(q, k.transpose(1, 2))\n",
        "        attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n",
        "\n",
        "        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_mask = attn_mask.unsqueeze(0)\n",
        "            attn_weights += attn_mask\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            # don't attend to padding symbols\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            if not is_tpu:\n",
        "                attn_weights = attn_weights.masked_fill(\n",
        "                    key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool),\n",
        "                    float(\"-inf\"),\n",
        "                )\n",
        "            else:\n",
        "                attn_weights = attn_weights.transpose(0, 2)\n",
        "                attn_weights = attn_weights.masked_fill(key_padding_mask, float(\"-inf\"))\n",
        "                attn_weights = attn_weights.transpose(0, 2)\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if before_softmax:\n",
        "            return attn_weights, v, position_bias\n",
        "\n",
        "        if position_bias is not None:\n",
        "            if self.gru_rel_pos == 1:\n",
        "                query_layer = q.view(bsz, self.num_heads, tgt_len, self.q_head_dim)\n",
        "                _B, _H, _L, __ = query_layer.size()\n",
        "                gate_a, gate_b = torch.sigmoid(self.grep_linear(query_layer).view(\n",
        "                    _B, _H, _L, 2, 4).sum(-1, keepdim=False)).chunk(2, dim=-1)\n",
        "                gate_a_1 = gate_a * (gate_b * self.grep_a - 1.0) + 2.0\n",
        "                position_bias = gate_a_1.view(bsz * self.num_heads, -1, 1) * position_bias\n",
        "\n",
        "            position_bias = position_bias.view(attn_weights.size())\n",
        "\n",
        "            attn_weights = attn_weights + position_bias\n",
        "\n",
        "        attn_weights_float = F.softmax(\n",
        "            attn_weights, dim=-1\n",
        "        )\n",
        "        attn_weights = attn_weights_float.type_as(attn_weights)\n",
        "        attn_probs = self.dropout_module(attn_weights)\n",
        "\n",
        "        assert v is not None\n",
        "        attn = torch.bmm(attn_probs, v)\n",
        "        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n",
        "        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
        "        attn = self.out_proj(attn)\n",
        "        attn_weights: Optional[Tensor] = None\n",
        "        if need_weights:\n",
        "            attn_weights = attn_weights_float.view(\n",
        "                bsz, self.num_heads, tgt_len, src_len\n",
        "            ).transpose(1, 0)\n",
        "            if not need_head_weights:\n",
        "                # average attention weights over heads\n",
        "                attn_weights = attn_weights.mean(dim=0)\n",
        "\n",
        "        return attn, attn_weights, position_bias\n",
        "\n",
        "    @staticmethod\n",
        "    def _append_prev_key_padding_mask(\n",
        "            key_padding_mask: Optional[Tensor],\n",
        "            prev_key_padding_mask: Optional[Tensor],\n",
        "            batch_size: int,\n",
        "            src_len: int,\n",
        "            static_kv: bool,\n",
        "    ) -> Optional[Tensor]:\n",
        "        # saved key padding masks have shape (bsz, seq_len)\n",
        "        if prev_key_padding_mask is not None and static_kv:\n",
        "            new_key_padding_mask = prev_key_padding_mask\n",
        "        elif prev_key_padding_mask is not None and key_padding_mask is not None:\n",
        "            new_key_padding_mask = torch.cat(\n",
        "                [prev_key_padding_mask.float(), key_padding_mask.float()], dim=1\n",
        "            )\n",
        "        # During incremental decoding, as the padding token enters and\n",
        "        # leaves the frame, there will be a time when prev or current\n",
        "        # is None\n",
        "        elif prev_key_padding_mask is not None:\n",
        "            if src_len > prev_key_padding_mask.size(1):\n",
        "                filler = torch.zeros(\n",
        "                    (batch_size, src_len - prev_key_padding_mask.size(1)),\n",
        "                    device=prev_key_padding_mask.device,\n",
        "                )\n",
        "                new_key_padding_mask = torch.cat(\n",
        "                    [prev_key_padding_mask.float(), filler.float()], dim=1\n",
        "                )\n",
        "            else:\n",
        "                new_key_padding_mask = prev_key_padding_mask.float()\n",
        "        elif key_padding_mask is not None:\n",
        "            if src_len > key_padding_mask.size(1):\n",
        "                filler = torch.zeros(\n",
        "                    (batch_size, src_len - key_padding_mask.size(1)),\n",
        "                    device=key_padding_mask.device,\n",
        "                )\n",
        "                new_key_padding_mask = torch.cat(\n",
        "                    [filler.float(), key_padding_mask.float()], dim=1\n",
        "                )\n",
        "            else:\n",
        "                new_key_padding_mask = key_padding_mask.float()\n",
        "        else:\n",
        "            new_key_padding_mask = prev_key_padding_mask\n",
        "        return new_key_padding_mask\n",
        "\n",
        "    def _get_input_buffer(\n",
        "            self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]\n",
        "    ) -> Dict[str, Optional[Tensor]]:\n",
        "        result = self.get_incremental_state(incremental_state, \"attn_state\")\n",
        "        if result is not None:\n",
        "            return result\n",
        "        else:\n",
        "            empty_result: Dict[str, Optional[Tensor]] = {}\n",
        "            return empty_result\n",
        "\n",
        "    def _set_input_buffer(\n",
        "            self,\n",
        "            incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n",
        "            buffer: Dict[str, Optional[Tensor]],\n",
        "    ):\n",
        "        return self.set_incremental_state(incremental_state, \"attn_state\", buffer)\n",
        "\n",
        "    def apply_sparse_mask(self, attn_weights, tgt_len: int, src_len: int, bsz: int):\n",
        "        return attn_weights"
      ],
      "metadata": {
        "id": "0E52HbHlRaE0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "hgU3dglBSCx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "gw3KWmCkRb2L"
      }
    }
  ]
}